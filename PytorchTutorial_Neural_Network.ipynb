{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PytorchTutorial_Neural.Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP2ioEf6jEiGURWecfTdzU3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toDJE1aU2NVJ",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network / 신경망\n",
        "\n",
        "An nn.Module contains layers, and a method forward(input)that returns the output.\n",
        "/ nn.Module 은 계층(layer)과 output 을 반환하는 forward(input) 메서드를 포함하고 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "![대체 텍스트](https://pytorch.org/tutorials/_images/mnist.png)\n",
        "\n",
        "\n",
        "> It is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "* Define the neural network that has some learnable parameters (or weights)\n",
        "* Iterate over a dataset of inputs\n",
        "* Process input through the network\n",
        "* Compute the loss (how far is the output from being correct)\n",
        "* Propagate gradients back into the network’s parameters\n",
        "* Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate * gradient\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        ">이는 간단한 순전파 네트워크(Feed-forward network)입니다. 입력(input)을 받아 여러 계층에 차례로 전달한 후, 최종 출력(output)을 제공합니다.\n",
        "\n",
        "신경망의 일반적인 학습 과정은 다음과 같습니다:\n",
        "\n",
        "* 학습 가능한 매개변수(또는 가중치(weight))를 갖는 신경망을 정의합니다.\n",
        "\n",
        "* 데이터셋(dataset) 입력을 반복합니다.\n",
        "\n",
        "* 입력을 신경망에서 전파(process)합니다.\n",
        "\n",
        "* 손실(loss; 출력이 정답으로부터 얼마나 떨어져있는지)을 계산합니다.\n",
        "\n",
        "* 변화도(gradient)를 신경망의 매개변수들에 역으로 전파합니다.\n",
        "\n",
        "* 신경망의 가중치를 갱신합니다. 일반적으로 다음과 같은 간단한 규칙을 사용합니다: 가중치(wiehgt) = 가중치(weight) - 학습율(learning rate) * 변화도(gradient)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEfq7dgx3Fag",
        "colab_type": "text"
      },
      "source": [
        "# Define the Network / 신경망 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CFQEB6p0xEn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "c4112aad-67a6-4342-be2a-b770a27aba60"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}