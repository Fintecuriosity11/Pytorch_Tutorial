{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PytorchTutorial_Neural Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMD4w1rbMMrpwKsF8X1Cs5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toDJE1aU2NVJ",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network / 신경망\n",
        "\n",
        "An nn.Module contains layers, and a method forward(input)that returns the output.\n",
        "/ nn.Module 은 계층(layer)과 output 을 반환하는 forward(input) 메서드를 포함하고 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "![대체 텍스트](https://pytorch.org/tutorials/_images/mnist.png)\n",
        "\n",
        "\n",
        "> It is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "* Define the neural network that has some learnable parameters (or weights)\n",
        "* Iterate over a dataset of inputs\n",
        "* Process input through the network\n",
        "* Compute the loss (how far is the output from being correct)\n",
        "* Propagate gradients back into the network’s parameters\n",
        "* Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate * gradient\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        ">이는 간단한 순전파 네트워크(Feed-forward network)입니다. 입력(input)을 받아 여러 계층에 차례로 전달한 후, 최종 출력(output)을 제공합니다.\n",
        "\n",
        "신경망의 일반적인 학습 과정은 다음과 같습니다:\n",
        "\n",
        "* 학습 가능한 매개변수(또는 가중치(weight))를 갖는 신경망을 정의합니다.\n",
        "\n",
        "* 데이터셋(dataset) 입력을 반복합니다.\n",
        "\n",
        "* 입력을 신경망에서 전파(process)합니다.\n",
        "\n",
        "* 손실(loss; 출력이 정답으로부터 얼마나 떨어져있는지)을 계산합니다.\n",
        "\n",
        "* 변화도(gradient)를 신경망의 매개변수들에 역으로 전파합니다.\n",
        "\n",
        "* 신경망의 가중치를 갱신합니다. 일반적으로 다음과 같은 간단한 규칙을 사용합니다: 가중치(wiehgt) = 가중치(weight) - 학습율(learning rate) * 변화도(gradient)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEfq7dgx3Fag",
        "colab_type": "text"
      },
      "source": [
        "# Define the Network / 신경망 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CFQEB6p0xEn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "c4112aad-67a6-4342-be2a-b770a27aba60"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):           \n",
        "        # Max pooling over a (2, 2) window\n",
        "        # Forward 함수 정의하면, (변화도 계산하는) backward 함수는 autograd를 사용하여 자동으로 정의\n",
        "\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ko56TOE4k_I",
        "colab_type": "text"
      },
      "source": [
        "The learnable parameters of a model are returned by net.parameters() / 모델의 학습 가능한 매개변수들은 net.parameters() 에 의해 반환됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNUt6Z0K4kvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b935bc8c-831f-4aff-f9f8-e0ccd1d17d67"
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "torch.Size([6, 1, 3, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFCeqHhr6HYM",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://www.pyimagesearch.com/wp-content/uploads/2016/06/lenet_architecture-768x226.png)\n",
        "\n",
        "Note: expected input size of this net (LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Note: 이 신경망(LeNet)의 예상되는 입력 크기는 32x32입니다. 이 신경망에 MNIST 데이터셋을 사용하기 위해서는, 데이터셋의 이미지 크기를 32x32로 변경해야 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apSSsRGf6G6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "945134a1-b3cf-456f-db4b-a899fa6cc2f1"
      },
      "source": [
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0221, -0.1019, -0.0370,  0.0806,  0.1099, -0.0173, -0.0500, -0.0744,\n",
            "         -0.0168, -0.0533]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBNFSsjc6rnf",
        "colab_type": "text"
      },
      "source": [
        "torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
        "\n",
        "For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width.\n",
        "\n",
        "If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.\n",
        "\n",
        "Before proceeding further, let’s recap all the classes you’ve seen so far.\n",
        "\n",
        "Recap:\n",
        "torch.Tensor - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
        "\n",
        "nn.Module - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
        "\n",
        "nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
        "\n",
        "autograd.Function - Implements forward and backward definitions of an autograd operation. Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history.\n",
        "\n",
        "At this point, we covered:\n",
        "Defining a neural network\n",
        "Processing inputs and calling backward\n",
        "\n",
        "Still Left:\n",
        "Computing the loss\n",
        "Updating the weights of the network\n",
        "\n",
        "---\n",
        "\n",
        "torch.nn 은 미니-배치(mini-batch)만 지원합니다. torch.nn 패키지 전체는 하나의 샘플이 아닌, 샘플들의 미니-배치만을 입력으로 받습니다.\n",
        "\n",
        "예를 들어, nnConv2D 는 nSamples x nChannels x Height x Width 의 4차원 Tensor를 입력으로 합니다.\n",
        "\n",
        "만약 하나의 샘플만 있다면, input.unsqueeze(0) 을 사용해서 가짜 차원을 추가합니다.\n",
        "\n",
        "계속 진행하기 전에, 지금까지 살펴봤던 것들을 다시 한번 요약해보겠습니다.\n",
        "\n",
        "\n",
        "요약:\n",
        "\n",
        "torch.Tensor - backward() 같은 autograd 연산을 지원하는 다차원 배열 입니다. 또한 tensor에 대한 변화도(gradient)를 갖고 있습니다.\n",
        "\n",
        "nn.Module - 신경망 모듈. 매개변수를 캡슐화(encapsulation)하는 간편한 방법 으로, GPU로 이동, 내보내기(exporting), 불러오기(loading) 등의 작업을 위한 헬퍼(helper)를 제공합니다.\n",
        "\n",
        "nn.Parameter - Tensor의 한 종류로, Module 에 속성으로 할당될 때 자동으로 매개변수로 등록 됩니다.\n",
        "\n",
        "autograd.Function - autograd 연산의 전방향과 역방향 정의 를 구현합니다. 모든 Tensor 연산은 하나 이상의 Function 노드를 생성하며, 각 노드는 Tensor 를 생성하고 이력(history)을 부호화 하는 함수들과 연결하고 있습니다.\n",
        "\n",
        "지금까지 우리가 다룬 내용은 다음과 같습니다:\n",
        "신경망을 정의하는 것\n",
        "\n",
        "입력을 처리하고 backward 를 호출하는 것\n",
        "\n",
        "더 살펴볼 내용들은 다음과 같습니다:\n",
        "손실을 계산하는 것\n",
        "\n",
        "신경망의 가중치를 갱신하는 것\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxdgtV7T8Dce",
        "colab_type": "text"
      },
      "source": [
        "# Loss Function / 손실 함수\n",
        "\n",
        "> A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
        "\n",
        "> There are several different loss functions under the nn package . A simple loss is: nn.MSELoss which computes the mean-squared error between the input and the target.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "> 손실 함수는 (output, target)을 한 쌍(pair)의 입력으로 받아, 출력(output)이 정답(target)으로부터 얼마나 멀리 떨어져있는지 추정하는 값을 계산합니다.\n",
        "\n",
        "> nn 패키지에는 여러가지의 손실 함수들 이 존재합니다. 간단한 손실 함수로는 출력과 대상간의 평균제곱오차(mean-squared error)를 계산하는 nn.MSEloss 가 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEGjL_408bPZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ec7c86e9-aaf8-4138-d46d-3d432a4d6836"
      },
      "source": [
        "output = net(input)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.1501, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i2W5jqi8iYP",
        "colab_type": "text"
      },
      "source": [
        "> Now, if you follow loss in the backward direction, using its .grad_fn attribute, you will see a graph of computations that looks like this:\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "> 이제 .grad_fn 속성을 사용하여 loss 를 역방향에서 따라가다보면, 이러한 모습의 연산 그래프를 볼 수 있습니다:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uy594hc81Yr",
        "colab_type": "text"
      },
      "source": [
        "> So, when we call loss.backward(), the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has requires_grad=True will have their .grad Tensor accumulated with the gradient.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> 따라서 loss.backward() 를 실행할 때, 전체 그래프는 손실(loss)에 대하여 미분되며, 그래프 내의 requires_grad=True 인 모든 Tensor는 변화도(gradient)가 누적된 .grad Tensor를 갖게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRiXTtOh8pqJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "926fc6ac-8fb9-4df6-8f68-d739a25ca67f"
      },
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MseLossBackward object at 0x7fe1d420deb8>\n",
            "<AddmmBackward object at 0x7fe1d420de48>\n",
            "<AccumulateGrad object at 0x7fe1d420deb8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhHvjUnh-KVF",
        "colab_type": "text"
      },
      "source": [
        "# Backprop / 역전파\n",
        "\n",
        "![대체 텍스트](https://miro.medium.com/max/1678/1*CxdjKFrE-Vww0KmI-3Z5sA.png)\n",
        "\n",
        "> To backpropagate the error all we have to do is to loss.backward(). You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "> 오차(error)를 역전파하기 위해서는 loss.backward() 만 해주면 됩니다. 기존 변화도를 없애는 작업이 필요한데, 그렇지 않으면 변화도가 기존의 것에 누적되기 때문입니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk8uzQwY-5Dh",
        "colab_type": "text"
      },
      "source": [
        "> Now we shall call loss.backward(), and have a look at conv1’s bias gradients before and after the backward.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> 이제 loss.backward() 를 호출하여 역전파 전과 후에 conv1의 bias gradient를 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob7Uy3ky-KCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "5daeb30b-e00c-44b2-f61a-910d18090637"
      },
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.bias.grad before backward\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "conv1.bias.grad after backward\n",
            "tensor([ 0.0027, -0.0131, -0.0141, -0.0238,  0.0044,  0.0293])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OlNBmoz_qI_",
        "colab_type": "text"
      },
      "source": [
        "# Update the Weights / 가중치 갱신\n",
        "\n",
        "> The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
        "\n",
        "> weight = weight - learning_rate * gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ-Q67MyAPwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "for f in net.parameters():\n",
        "    f.data.sub_(f.grad.data * learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFvd7WnZAIjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}