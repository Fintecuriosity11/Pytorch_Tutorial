{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PytorchTutorial_Autograd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOI4wabxqK4gUqaSK1LrhBW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un0zj_iGiLjj",
        "colab_type": "text"
      },
      "source": [
        "# AUTOGRAD: AUTOMATIC DIFFERENTIATION / 자동 미분\n",
        "\n",
        "> The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
        "\n",
        "> autograd 패키지는 Tensor의 모든 연산에 대해 자동 미분을 제공합니다. 이는 실행-기반-정의(define-by-run) 프레임워크로, 이는 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 뜻이며, 역전파는 학습 과정의 매 단계마다 달라집니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEV9x4k5i3Ut",
        "colab_type": "text"
      },
      "source": [
        "# Tensor\n",
        "\n",
        "Create a tensor and set requires_grad=True to track computation with it / tensor를 생성하고 requires_grad=True 를 설정하여 연산을 기록합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_lPvdKgh3Ep",
        "colab_type": "code",
        "outputId": "3a610664-766c-4178-83fb-1f91d7931f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import torch                                     # Always import \"torch\" package first\n",
        "\n",
        "x = torch.ones(2, 2, requires_grad=True)         # requires_grad는 \n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP7Da5BrlHgV",
        "colab_type": "text"
      },
      "source": [
        "Do a tensor operation: / tensor에 연산을 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3dMWPYFlHm4",
        "colab_type": "code",
        "outputId": "cfb09c53-6bc7-43f8-d256-5c7c5df1dc34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "y = x + 2\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geCTDyc1mudV",
        "colab_type": "text"
      },
      "source": [
        "y was created as a result of an operation, so it has a grad_fn. / y 는 연산의 결과로 생성된 것이므로 grad_fn을 갖는다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjpkRRmimuoa",
        "colab_type": "code",
        "outputId": "4498de81-9b41-4b7b-f13d-c261e48bcace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(y.grad_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<AddBackward0 object at 0x7fea76b95470>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLp8ibkWq_nH",
        "colab_type": "text"
      },
      "source": [
        "Do more operations on y / y 에 다른 연산을 수행\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSxoXe9MrIAD",
        "colab_type": "code",
        "outputId": "35b5d434-bbea-4049-ca0d-e68348c2ddee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(z, out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6VjpFMfrPyn",
        "colab_type": "text"
      },
      "source": [
        ".requires_grad_( ... ) changes an existing Tensor’s requires_grad flag in-place. The input flag defaults to False if not given. / .requires_grad_( ... ) 는 기존 Tensor의 requires_grad 값을 바꿔치기 (in-place)하여 변경 입력값이 지정되지 않으면 기본값은 False "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_bWv1atrWRf",
        "colab_type": "code",
        "outputId": "f48d7775-faa4-401b-f5a1-689419a53038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "a = torch.randn(2, 2)\n",
        "a = ((a * 3) / (a - 1))\n",
        "\n",
        "print(a.requires_grad)\n",
        "\n",
        "a.requires_grad_(True)\n",
        "\n",
        "print(a.requires_grad)\n",
        "\n",
        "b = (a * a).sum()\n",
        "\n",
        "print(b.grad_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "<SumBackward0 object at 0x7fea76bfc860>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzg199UNrwf5",
        "colab_type": "text"
      },
      "source": [
        "# Gradients / 변화도\n",
        "\n",
        "이제 역전파(backprop)를 실행. out 은 하나의 스칼라 값만 갖고 있기 때문에, out.backward() 는 out.backward(torch.tensor(1.)) 과 동일"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1oUmumArwyj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4352a058-c7a0-4b40-903b-646fb0408dba"
      },
      "source": [
        "out.backward()\n",
        "\n",
        "print(x.grad) # Print gradients d(out)/dx"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjbmWkCht5ja",
        "colab_type": "text"
      },
      "source": [
        "* # Vector-Jacobian product / 벡터-야코비안 곱\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07EqklCZt5Tz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7cbc06ea-e58b-46f1-b2ac-80ad945b24a7"
      },
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "while y.data.norm() < 1000:\n",
        "    y = y * 2\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 1030.6512, -1299.9962,   805.2855], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNtQ3yp6usdo",
        "colab_type": "text"
      },
      "source": [
        "Now in this case y is no longer a scalar. torch.autograd could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to backward as argument  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "이 경우 y 는 더 이상 스칼라 값이 아닙니다. torch.autograd 는 전체 야코비안을 직접 계산할수는 없지만, 벡터-야코비안 곱은 간단히 backward 에 해당 벡터를 인자로 제공하여 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpSOSznIusCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8771d130-cda0-4720-bd38-29c2808c1bf8"
      },
      "source": [
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # tensor의 shape와 같음\n",
        "y.backward(v)\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvrVxL1jwq49",
        "colab_type": "text"
      },
      "source": [
        "You can also stop autograd from tracking history on Tensors with .requires_grad=True either by wrapping the code block in with torch.no_grad():\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "또한 with torch.no_grad(): 로 코드 블럭을 감싸서 autograd가 .requires_grad=True 인 Tensor들의 연산 기록을 추적하는 것을 멈출 수 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkt1CaQAwrDa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9509667c-4f9f-4b8e-d26f-d937576b303a"
      },
      "source": [
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    print((x ** 2).requires_grad)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_EhXq51xnfK",
        "colab_type": "text"
      },
      "source": [
        "Or by using .detach() to get a new Tensor with the same content but that does not require gradients:\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "또는 .detach() 를 호출하여 내용물(content)은 같지만 require_grad가 다른 새로운 Tensor를 가져옵니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvnmJSboxnBu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c0fc117b-b69a-413b-9881-53228aadb558"
      },
      "source": [
        "print(x.requires_grad)\n",
        "y = x.detach()\n",
        "print(y.requires_grad)\n",
        "print(x.eq(y).all())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}